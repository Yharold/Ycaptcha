{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 项目分析\n",
    "\n",
    "这是一个验证码识别项目，原本的项目仓库是![GiantPandaCV验证码识别竞赛解决方案](https://github.com/pprp/captcha.Pytorch)。主要的目标就是创建一个模型，训练模型，然后测试模型的准确率。需要实现的功能有：\n",
    "1. 数据增强：需要对原始的数据进行数据增强，增多模型的训练数据。\n",
    "2. 模型创建：需要创建多个模型，分别验证不同模型的效果。\n",
    "3. 损失函数：对多种损失函数进行验证\n",
    "4. 优化器：对多个优化器进行验证\n",
    "5. 学习率：对学习率进行验证\n",
    "6. 训练和测试：训练模型，保存模型，并测试模型的准确率。 \n",
    "\n",
    "## 数据增强\n",
    "数据增强的方式有很多种，旋转，裁剪，缩放，翻转，噪声等。原始的数据增强方式有5种，分别是：\n",
    "- 扭曲\n",
    "- 数据9:1划分+扩增3倍\n",
    "- 扭曲+缩放\n",
    "- 倾斜+扩增两倍 \n",
    "- 扭曲+缩放+倾斜+扩增两倍\n",
    "- 9:1划分+倾斜 \n",
    "原始数据训练集和测试集是8:2划分，所以这里的9:1同样是对数据的扩增。 \n",
    "\n",
    "我这里只用一种方式，就是扭曲+缩放。文件是utils/dataAug.py。这个文件中实现了两种数据增强方式，扭曲+缩放和扭曲+缩放+倾斜。\n",
    "\n",
    "## 模型创建\n",
    "原始的模型只使用ResNet18和ResNet34，但后面作者说会测试attention，ibn，bnneck等。作者这里的Resnet18模型有一些修改，第一个卷积变成了3x3的大小，去掉了最大池化层，添加了dropout层，最后通过4个全连接层输出4个字符。\n",
    "\n",
    "我这里的模型是使用了作者的ResNet18和原生的ResNet18，主要目标是做一个对比。\n",
    "\n",
    "## 损失函数\n",
    "损失函数就用交叉熵损失函数(CrossEntropyLoss)即可。\n",
    "\n",
    "## 优化器\n",
    "作者使用的优化器有三个，Adam，RAdam和AdamW。\n",
    "我这里使用RAdam和AdamW。目的依旧是为了做一个对比\n",
    "\n",
    "## 学习率\n",
    "作者这里的学习率有多个，具体可以看最后的那张表。我这里使用的学习率是0.001，调度器的类是在lib/scheduler.py。它使用warmup和weight decay。\n",
    "\n",
    "## 训练和测试\n",
    "\n",
    "训练和测试需要注意的就是如何保存记录。这里我就使用TensorBoard记录训练过程。另外就是使用GPU加速训练。\n",
    "\n",
    "# 项目结构\n",
    "项目结构如下：\n",
    "- root:\n",
    "  - datasets:存放数据\n",
    "    - train：训练数据\n",
    "    - test：测试数据\n",
    "    - auged_train_0：第一种数据增强方式\n",
    "    - auged_train_1：第二种数据增强方式\n",
    "  - logs：存放TensorBoard记录\n",
    "  - models：存放模型类\n",
    "    - model.py: 模型类\n",
    "  - weights：存放模型权重\n",
    "  - config: 存放配置文件\n",
    "    - parameter.py：参数配置,包括数据集路径，训练轮数，批量大小，学习率，模型保存路径等。\n",
    "  - utils: 存放工具类\n",
    "    -  dataAug.py: 数据增强类\n",
    "  - lib: 存放损失函数，优化器，调度器，读取数据等\n",
    "    - loss.py：损失函数\n",
    "    - optimizer.py：优化器\n",
    "    - scheduler.py：学习率调度器\n",
    "    - dataset.py：读取数据 \n",
    "  - train.py：训练脚本\n",
    "  - test.py：测试脚本\n",
    "  - predict.py：预测脚本\n",
    "  - main.py：主函数\n",
    "\n",
    "# 实验计划\n",
    "训练分为下面几种情况：\n",
    "1 RawResNet+lr=0.001+epoch=200+batch_size=64+RAdam+auged0=64 \n",
    "2 ResNet+lr=0.001+epoch=200+batch_size=64+RAdam+auged0=64 \n",
    "3 ResNet+lr=0.001+epoch=200+batch_size=64+RAdam+auged1=64\n",
    "4 ResNet+lr=0.001+epoch=200+batch_size=64+AdamW+auged0=64 \n",
    "5 ResNet+lr=0.0035+epoch=200+batch_size=64+RAdam+auged0=64 \n",
    "6 ResNet+lr=0.0005+epoch=200+batch_size=64+RAdam+auged0=64 \n",
    "\n",
    "1 2对比选出最优的模型，23对比选出最好的数据增强方式，24对比选出最好的优化器，2,5,6对比选择好的学习率。\n",
    "实际是做完一个对比才能有下一个对比。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RAdam\n",
    "\n",
    "RAdam是一种改进的Adam优化器，论文地址是![RAdam论文地址]https://arxiv.org/pdf/1908.03265.pdf\n",
    "具体的算法如下：\n",
    "![RAdam算法](fig/Radam.png)\n",
    "\n",
    "下面是RAdam的实现代码"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import torch\n",
    "from torch.optim import Optimizer\n",
    "\n",
    "class RAdam(Optimizer):\n",
    "    def __init__(self, params, lr=1e-3, betas=(0.9, 0.999), eps=1e-8, weight_decay=0):\n",
    "        defaults = dict(lr=lr, betas=betas, eps=eps, weight_decay=weight_decay)\n",
    "        # 用来保存公式中的一些中间量，这些量和梯度五官，任意权重计算时都是相同的，所以保存下来减少计算量\n",
    "        self.buffer = [[None, None, None] for _ in range(10)]\n",
    "        super(RAdam, self).__init__(params, defaults)\n",
    "\n",
    "    def __setstate__(self, state):\n",
    "        super(RAdam, self).__setstate__(state)\n",
    "\n",
    "    def step(self, closure=None):\n",
    "        loss = None\n",
    "        if closure is not None:\n",
    "            loss = closure()\n",
    "        for group in self.param_groups:\n",
    "            for p in group[\"params\"]:\n",
    "                if p.grad is None:\n",
    "                    continue\n",
    "                grad = p.grad.data.float()\n",
    "                if grad.is_sparse:\n",
    "                    raise RuntimeError(\"RAdam does not support sparse gradients\")\n",
    "                p_data_fp32 = p.data.float()\n",
    "                state = self.state[p]\n",
    "                if len(state) == 0:\n",
    "                    # state['step'] 保存的就是时刻t\n",
    "                    # state['exp_avg'] 保存的是mt\n",
    "                    # state['exp_avg_sq'] 保存的是vt\n",
    "                    state[\"step\"] = 0\n",
    "                    state[\"exp_avg\"] = torch.zeros_like(p_data_fp32)\n",
    "                    state[\"exp_avg_sq\"] = torch.zeros_like(p_data_fp32)\n",
    "                else:\n",
    "                    state[\"exp_avg\"] = state[\"exp_avg\"].type_as(p_data_fp32)\n",
    "                    state[\"exp_avg_sq\"] = state[\"exp_avg_sq\"].type_as(p_data_fp32)\n",
    "                exp_avg, exp_avg_sq = state[\"exp_avg\"], state[\"exp_avg_sq\"]\n",
    "                beta1, beta2 = group[\"betas\"]\n",
    "                # mul_就是乘，addcmul_就是加权乘，add_就是加 \n",
    "                # 这两步就是计算vt和mt\n",
    "                exp_avg_sq.mul_(beta2).addcmul_(1 - beta2, grad, grad)\n",
    "                exp_avg.mul_(beta1).add_(1 - beta1, grad)\n",
    "                state[\"step\"] += 1\n",
    "                buffered = self.buffer[int(state[\"step\"] % 10)]\n",
    "                if state[\"step\"] == buffered[0]:\n",
    "                    N_sma, step_size = buffered[1], buffered[2]\n",
    "                else:\n",
    "                    buffered[0] = state[\"step\"]\n",
    "                    # beta2_t 就是beta2的t次方，t就是时间t\n",
    "                    beta2_t = beta2 ** state[\"step\"]\n",
    "                    # N_sma_max就是公式中的rho无穷大\n",
    "                    N_sma_max = 2 / (1 - beta2) - 1\n",
    "                    # N_sma就是公式中的rho_t\n",
    "                    N_sma = N_sma_max - 2 * state[\"step\"] * beta2_t / (1 - beta2_t)\n",
    "                    buffered[1] = N_sma\n",
    "                    # 原论文中是大于4，这里是大于等于5\n",
    "                    if N_sma >= 5:\n",
    "                        # step_size是将公式中的一些不含梯度的变量(比如beta1，beta2等)进行了合并计算\n",
    "                        # 因为这个数和梯度无关，所以任何一个参数计算时都是相同的，-\n",
    "                        # 所以这里用了buffer来保存这个变量，这样可以减少计算量\n",
    "                        step_size = (\n",
    "                            group[\"lr\"]\n",
    "                            * math.sqrt(\n",
    "                                (1 - beta2_t)\n",
    "                                * (N_sma - 4)\n",
    "                                / (N_sma_max - 4)\n",
    "                                * (N_sma - 2)\n",
    "                                / N_sma\n",
    "                                * N_sma_max\n",
    "                                / (N_sma_max - 2)\n",
    "                            )\n",
    "                            / (1 - beta1 ** state[\"step\"])\n",
    "                        )\n",
    "                    else:\n",
    "                        step_size = group[\"lr\"] / (1 - beta1 ** state[\"step\"])\n",
    "                    buffered[2] = step_size\n",
    "                # 计算有衰减时的值\n",
    "                if group[\"weight_decay\"] != 0:\n",
    "                    p_data_fp32.add_(-group[\"weight_decay\"] * group[\"lr\"], p_data_fp32)\n",
    "                # 更新权重\n",
    "                if N_sma >= 5:\n",
    "                    denom = exp_avg_sq.sqrt().add_(group[\"eps\"])\n",
    "                    p_data_fp32.addcdiv_(-step_size, exp_avg, denom)\n",
    "                else:\n",
    "                    p_data_fp32.add_(-step_size, exp_avg)\n",
    "                # 复制到p.data中\n",
    "                p.data.copy_(p_data_fp32)\n",
    "        return loss"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DL",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
